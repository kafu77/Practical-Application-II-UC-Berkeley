# Practical-Application-II-UC-Berkeley
The practical application project II followed the most used methodology for data mining, analytics, and data science projects called a Cross Industry Standard Process for Data Mining (CRISP-DM). The CRISP-DM used to identify what used cars are most common consumable cars and at the end used to recommend to the used cars dealership clients. The data collected from online data storage website called Kaggle. The dataset originally was about 3 million, but for this specific project only 426880 rows and 18 features are used. The describe method is used to compute the statistical properties of the dataset. This type of method computed only for the numerical features. The dataset has missing and outliers. A small code has been used to compute the percentage of the missing value. Some, features have large missing value and others are small. Even though some features have large percentage of missing value, they are more significant features to the model to predict the price.
After our initial exploration and fine tuning of the business understanding, and data understanding, the cleaning tools have been utilized to clean the data set. The project starts to check and clean the missing values. The method extracting the relevant content from a data by removing the outliers. After the outlier have been removed, only used cars that have a price of between 500 to 120,000, made from 1900 to 2021, and travelled between 63318 to 280,000 miles are used for this project. Then, dropped out all the less important features and the missing value by using a dropna method, then the dataset dropped to 111214 .  rows and 14 columns.

After cleaning the data, the data visualization libraries are used to visualize the correlation of the data. A seaborn and multiplot are the two most data visualization libraries used in this project. The seaborn distribution plot, categorical plot, and matrix plot are used to visualize the date set. A seaborn count plot counts the used cars by the features. California state has the highest number of used cars 45131 followed by Florida state which has 25790 number of used cars. The ford company is the highest company utilize a used cars. The count plot also used for the number of cylinders, condition, transmission, and type to counts. Then used a boxplot to see the average used cars by the features and other seaborn plot tools. The seaborn heatmap shows the correlation of the numerical features. From the plot the numerical data set has a less correlation to each other. A cluster map also applied to see the marginal plots in addition the heatmap plot.


Finally, after collecting, exploring, describing, and cleaning the dataset, a multilinear regression model is applied. The two models applied in this project are linear regression model and ridge model. First the dataset categorized into numerical and categorical dataset. The categorical data set transformed into the binary numbers using OneHotEnocder and transformed into numbers using a LabelEncoder. Then the numerical and categorical dataset concatenated to normalize. A StandardScaler method is the scaler method to normalize the dataset. The data set has 111214 rows and 150 columns. Since, after the categorical features transformed into binary increase the number of features (columns), a principal component analysis (PCA) applied to the normalized data to reduce the number of columns without losing any information. Then, computed the most significant number of components. 140 numbers of principal components give 1% variance ratio, only 140 principal components are used. Eventually, split the data set into train and test data set. The train data sets are 70% and 30% of the test data set. After splitting the dataset, a baseline model is prepared to compare the results. The baseline model has a 1.428575694760455 train mean square error and 2.099756126855142e-06 test mean square error.

Before fitting the data into a model, a polynomial feature extraction technique is used to identify the best degree for the model. Because of the computer memory limitation, only a range 1 to 5 degrees are tested. The model gets degree 3 is the best polynomials degree for this dataset. Then, a degree 3 polynomial is used. After the higher order selected, a sequential feature selection model and ridge model are utilized. A sequential feature selection with LASSO estimator, forward direction, and k fold cross validation is used. A linear regression model is trained and fitted to the data set. Then, the model predicts a 1.42855556358884 train mean square error and 6.03199297773313e-05 test mean square error. The linear regression model predicts a less train mean square error than the baseline model and a higher test mean square error. Again, A Ridge model is applied to the data set. This model has a 1.4241372980438263 train mean square error and 0.004757492811698727 test mean square error. The Ridge model train mean square error is smaller than the sequential feature selection model and baseline model and test mean square error is higher than the sequential feature selection model

