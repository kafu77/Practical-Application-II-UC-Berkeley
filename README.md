# Practical-Application-II-UC-Berkeley
The project used the CRISP-DM methodology for data mining and analytics. The goal was to identify the most commonly consumed used cars and make recommendations to a dealership's clients. The dataset was obtained from Kaggle and had 426880 rows and 18 features. The describe method was used to compute statistical properties of the numerical features, and missing and outlier values were handled using cleaning tools. The dataset was cleaned by removing outliers and dropping less important features and missing values. The final dataset had 111214 rows and 14 columns, containing only used cars with a price between 500 to 120,000, made from 1900 to 2021, and travelled between 63318 to 280,000 miles.

After cleaning the data, data visualization libraries were used to visualize the correlation in the dataset. Seaborn and multiplot were used, along with distribution plots, categorical plots, and matrix plots. A count plot was used to count the used cars by features, and California had the highest number of used cars, followed by Florida. The boxplot was used to see the average used cars by features, and the heatmap showed the correlation of the numerical features. The cluster map was used to see marginal plots in addition to the heatmap plot. The numerical data set had low correlation with each other.


After collecting, exploring, describing, and cleaning the dataset, a multilinear regression model was applied, including linear and ridge models. The dataset was categorized into numerical and categorical, and the categorical dataset was transformed into binary using OneHotEncoder and LabelEncoder. The datasets were concatenated and normalized using StandardScaler. Principal component analysis (PCA) was used to reduce the number of columns, and only 140 principal components were used. The dataset was split into train and test datasets (70% and 30%, respectively), and a baseline model was prepared to compare the results. The baseline model had a 1.428575694760455 train mean square error and 2.099756126855142e-06 test mean square error.

Before fitting the data into a model, a polynomial feature extraction technique was used to identify the best degree for the model. Degree 3 was found to be the best for this dataset. A sequential feature selection model and Ridge model were then utilized, using LASSO estimator, forward direction, and k-fold cross-validation. A linear regression model was trained and fitted to the dataset, predicting a train mean square error of 1.42855556358884 and a test mean square error of 6.03199297773313e-05. The Ridge model was also applied, producing a train mean square error of 1.4241372980438263 and a test mean square error of 0.004757492811698727. The Ridge model had a smaller train mean square error than the sequential feature selection model and baseline model, but a higher test mean square error than the sequential feature selection model.

Conclusion:
The conclusion of the project states that despite the limitations of computer memory, the models executed the best results based on the given hyperparameters. The best model for this project is the multi-linear regression model with a degree of 3 and sequential feature selection using an estimator LASSO. This model can be used to predict for any used car data set.

